{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**ids-pdl11-tut.ipynb**: This Jupyter notebook is provided by Joachim Vogt for the *Python Data Lab* of the module *CH-700 Introduction to Data Science* offered in Fall 2023 at Constructor University. Jupyter notebooks and other learning resources are available from a dedicated *module platform*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial explains how the Python package scikit-learn is applied to fundamental Machine Learning (ML) tasks. It builds on the companion tutorial *First steps with scikit-learn*. Follow the instructions below to learn to\n",
    "\n",
    "- [ ] explain key ML applications in the categories of supervised and unsupervised learning,\n",
    "- [ ] carry out basic classification and regression tasks using scikit-learn,\n",
    "- [ ] employ basic clustering methods using scikit-learn,\n",
    "- [ ] apply and appraise principal component analysis (PCA) for dimensionality reduction,\n",
    "- [ ] understand important model validation terminology and tools.\n",
    "\n",
    "If you wish to keep track of your progress, you may edit this markdown cell, check a box in the list above after having worked through the respective part of this notebook, and save the file.\n",
    "\n",
    "*Short exercises* are embedded in this notebook. *Sample solutions* can be found at the end of the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following data files are provided with scikit-learn.\n",
    "\n",
    "- [Iris flower data set](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html):  multivariate data set consisting of three different types of irisesâ€™ (Setosa, Versicolour, and Virginica) petal and sepal length, used here for classification and clustering.\n",
    "- [Hand-written digits](https://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html): 8x8 pixel images of digits, used here for classification and dimensionality reduction.\n",
    "\n",
    "Furthermore, the following data file is expected to reside in the working directory. Identify the file on the module platform and upload it to the same folder as this Jupyter notebook.\n",
    "\n",
    "- `co2_mm_mlo.txt`: [Mauna Loa CO$_2$ monthly mean data](https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_mm_mlo.txt) hosted by [NOAA's Global Monitoring Laboratory](https://gml.noaa.gov/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code cell to import standard Python data science libraries. The NumPy module facilitates efficient processing of numerical arrays, and is usually imported as `np`. The Pandas module facilitates display and processing of tabular data. From the matplotlib library we import the package `pyplot` using the standard abbreviation `plt`. The magic command `%matplotlib inline` (IPython shell) allows for inline display of graphics. The Seaborn package provides additional statistical visualization tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Python package scikit-learn is imported as `sklearn`. Dedicated modules providing model classes (estimator objects) are imported when needed, see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scikit-learn documentation and tutorials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scikit-learn package is hosted at [scikit-learn.org](https://scikit-learn.org/stable/index.html), with an extensive [user guide](https://scikit-learn.org/stable/user_guide.html) and [tutorials](https://scikit-learn.org/stable/tutorial/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic ML vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A basic distinction in ML is between *supervised learning* and *unsupervised learning*. In both categories, the scikit-learn package assumes on input a number of *samples* with specific *features* in the form of a *features matrix* `X` of shape `(n_samples, n_features)`. In addition, supervised learning tasks assume a *target vector* `y` of shape `(n_samples,)`, see below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In supervised learning, a predictive model is built from input data that can be divided into a set of features (predictor variables) and target values that are to be closely matched by the model predictions, i.e., by the output of the model when it is evaluated on the corresponding features. If the nature of the predictions is discrete (labels, names), the supervised learning task is called *classification*, otherwise, when predictions take values from a continuous range, *regression*. Input data supplemented by such target values are sometimes referred to as *labeled data*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In scikit-learn, an estimator (named `model` in the following list) for a supervised learning task comes with the following generic methods.\n",
    "\n",
    "- `model.fit()`: fit the training data to find the instance of the model class that is closest to the observations. As arguments, the features matrix `X` and the target vector `y` must be provided.\n",
    "- `model.predict()`: once a specific instance of the model is found, compute predictions for a given set of predictor variables.\n",
    "- `model.score()`: measure the fit quality, usually in the range between 0 and 1, with better fits indicated by larger values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In unsupervised learning, the input data do not contain separate target values to be matched by the model predictions, and may be thus referred to as *unlabeled data*. Patterns and connections are supposed to be inferred from a suitable metrics capturing associations between the observations. Unsupervised learning applications are *clustering* and *dimensionality reduction*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In scikit-learn, an estimator (named `model` in the description below) for an unsupervised learning task is equipped with the generic method `fit()`.\n",
    "\n",
    "- `model.fit()`: build a model based on the training data, with the features matrix `X` as the only argument.\n",
    "\n",
    "In unsupervised learning tasks such as clustering, a predictive model is built. Then the method `predict()` is available, possibly also in combination with `fit()` to yield a single function `fit_predict()`.\n",
    "\n",
    "- `model.predict()`: once a specific instance of the model is found, compute predictions for a given set of predictor variables.\n",
    "\n",
    "Data transformations employed for dimensionality reduction and related tasks come with the method `transform()`. The combination `fit_transform()` may be provided not only for convenience but also for computational efficiency. The method  `inverse_transform()` produces a representation of a possibly reduced (or compressed) data set in the original frame of reference. \n",
    "\n",
    "- `model.transform()`: transform data using the representation generated by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression examples are discussed in the companion tutorial *First steps with scikit-learn*. For completeness, essential steps of the Mauna Loa CO$_2$ fit exercises are reproduced in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Select the model class.\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model0 = LinearRegression(fit_intercept=False)\n",
    "model1 = LinearRegression(fit_intercept=False)\n",
    "model2 = LinearRegression(fit_intercept=False)\n",
    "### Load data, construct target vector.\n",
    "time,y = np.genfromtxt('co2_mm_mlo.txt',usecols=(2,3),unpack=True)\n",
    "### Construct features matrices.\n",
    "X0 = np.ones(time.size)[:,np.newaxis]                   #.. fit to a constant\n",
    "X1 = np.stack([np.ones(time.size),time],axis=1)         #.. linear fit\n",
    "X2 = np.stack([np.ones(time.size),time,time**2],axis=1) #.. quadratic fit\n",
    "### Split data for training and testing/validation.\n",
    "from sklearn.model_selection import train_test_split\n",
    "X0_trn,X0_val,y0_trn,y0_val = train_test_split(X0,y,train_size=0.5,random_state=99)\n",
    "X1_trn,X1_val,y1_trn,y1_val = train_test_split(X1,y,train_size=0.5,random_state=99)\n",
    "X2_trn,X2_val,y2_trn,y2_val = train_test_split(X2,y,train_size=0.5,random_state=99)\n",
    "### Train the models.\n",
    "model0.fit(X0_trn,y0_trn)\n",
    "model1.fit(X1_trn,y1_trn)\n",
    "model2.fit(X2_trn,y2_trn)\n",
    "### Compute predictions using features for testing.\n",
    "y0_prd = model0.predict(X0_val)\n",
    "y1_prd = model1.predict(X1_val)\n",
    "y2_prd = model2.predict(X2_val)\n",
    "### Compute validation scores (R^2 relative to mean value).\n",
    "print('Constant function,  validation score : ',model0.score(X0_val,y0_val))\n",
    "print('Linear function,    validation score : ',model1.score(X1_val,y1_val))\n",
    "print('Quadratic function, validation score : ',model2.score(X2_val,y2_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification is a supervised learning task using labeled data with discrete target values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Classification of the iris flower data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [iris flower data set](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html) is used to demonstrate classification with scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select the model class\n",
    "\n",
    "Following the procedure explained in the regression example, we build a predictive model using the Gaussian naive Bayes classifier as described in the [scikit-learn user guide](https://scikit-learn.org/stable/user_guide.html) in the section on [Naive Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Provide the data in an appropriate format\n",
    "\n",
    "Setting the keyword `return_X_y` to `True`, the scikit-learn convenience function `datasets.load_iris()` returns the iris flower data set nicely partitioned into a features matrix `X` and a target vector `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = datasets.load_iris(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The elements of the target vector `y` are labels 0,1,2 indicating the three types of iris flowers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the model to the data\n",
    "\n",
    "The chosen classification model is fitted to the data provided in the form of the features matrix `X` and target vector `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X,y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute model predictions\n",
    "\n",
    "Model predictions are computed and displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yprd = model.predict(X)\n",
    "print('Predicted labels:')\n",
    "print(yprd)\n",
    "print('\\nCorrect predictions:')\n",
    "print(y==yprd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Except for six entries, the labels are predicted correctly. In a more concise manner, classification results are summarized in the *confusion matrix*, with true labels in rows (vertical axis) and predicted labels in columns (horizontal axis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "#confusion_matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix is computed and displayed for the iris flower model generated using the Gaussian Naive Bayes estimator. Three observations labeled as category 1 are incorrectly classified as category 2 (array element in row 1 and column 2), and three other observations labeled as category 2 are incorrectly classified as category 1 (array element in row 2 and column 1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfMat = confusion_matrix(y,yprd)\n",
    "print(ConfMat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand why misclassifications happen only among species 1 and 2 but not with species 0, let us display  bivariate distributions for all pairs of features in the form of scatter plots using the Seaborn function `pairplot()`. To this end, the data are re-loaded into Pandas DataFrame objects, and the species labels are added to the features matrix as a new column. The diagrams show that there is some overlap between species 1 and 2 whereas each of them is well separated from species 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IrisDF,yDF = datasets.load_iris(return_X_y=True,as_frame=True)\n",
    "IrisDF['species'] = yDF\n",
    "sns.pairplot(IrisDF,hue='species',height=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model (cross-)validation\n",
    "\n",
    "The model could be validated using holdout sets in a similar manner as for the regression example. Here we opt for an alternative and more refined approach, namely, cross-validated metrics as described in the [scikit-learn user guide](https://scikit-learn.org/stable/user_guide.html) in the section on [Cross validation](https://scikit-learn.org/stable/modules/cross_validation.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "#cross_val_score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The keyword argument `cv` specifies the number of subsamples used for cross validation. With `cv=5`, the data are divided into five subsamples of which four are used for training and one for testing, resulting in five different possible combinations and thus (cross) validation scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = cross_val_score(model,X,y,cv=5)\n",
    "print(cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Classification of the reduced hand-written digits data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the first three classes of the [hand-written digits data set](https://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html), then carry out classification in the same way as for the iris flower example.\n",
    "\n",
    "- Select Gaussian naive Bayes as a classifier.\n",
    "- Load the data into a features matrix `X` and a target vector `y` (there is no need to print `y`).\n",
    "- Fit the model to the data.\n",
    "- Compute model predictions `yprd` and the confusion matrix (there is no need to print `yprd` or `y==yprd`, or to visualize the distributions using `pairplot()`).\n",
    "- Evaluate the model by means of cross-validation scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Select the model class.\n",
    "\n",
    "### Provide the data in an appropriate format.\n",
    "\n",
    "### Fit the model to the data.\n",
    "\n",
    "### Compute model predictions and the confusion matrix.\n",
    "\n",
    "### Model (cross-)validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task of assigning unlabeled data to a number of discrete groups is commonly referred to as *clustering*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Clustering of the iris flower data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate the workflow of a clustering task, the [Iris flower data set](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html) is revisited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select the model class\n",
    "\n",
    "The clustering algorithm we choose is `AgglomerativeClustering`, see the description of [Hierarchical clustering](https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering) as part of the section on [Clustering](https://scikit-learn.org/stable/modules/clustering.html) in the [scikit-learn user guide](https://scikit-learn.org/stable/user_guide.html). Assuming it is known a priori that three categories of flowers are present in the data, the number of cluster is appropriately chosen through the keyword `n_clusters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "model = AgglomerativeClustering(n_clusters=3)\n",
    "#model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Provide the data in an appropriate format\n",
    "\n",
    "As explained in the classification example, the scikit-learn convenience function `datasets.load_iris()` separates the features matrix `X` from the target vector. The latter is ignored as we deal with an unsupervised learning task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,_ = datasets.load_iris(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the model to the data\n",
    "\n",
    "The model is fitted to the data by means of the method `fit()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster labels are stored in the model attribute `labels_`. Alternatively, the array of labels may be obtained as a return variable of the `fit_predict()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize how the cluster labels relate to the data, we again employ the Seaborn function `pairplot()` to produce bivariate distributions. Cluster labels are added to the Pandas DataFrame object as a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IrisDF,yDF = datasets.load_iris(return_X_y=True,as_frame=True)\n",
    "IrisDF['cluster'] = model.labels_\n",
    "sns.pairplot(IrisDF,hue='cluster',height=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distributions of cluster labels are very similar to those of species labels, with cluster 0 corresponding to species 1, cluster 1 to species 0, and cluster 2 to species 2. The correspondences can be summarized by means of the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MatSpCl = confusion_matrix(yDF.values,model.labels_)\n",
    "print(MatSpCl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consult the section on [Clustering](https://scikit-learn.org/stable/modules/clustering.html) in the [scikit-learn user guide](https://scikit-learn.org/stable/user_guide.html) to learn which clustering algorithms are implemented in scikit-learn, and how they perform on different types of data sets. In particular, familiarize with basic clustering methods such as k-Means or Gaussian Mixture Models.\n",
    "\n",
    "In the clustering example above, choose other algorithms and vary the number of clusters. Observe how the results change. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When data sets are understood as points or vectors in a data space, the number of features corresponds to the space dimension, and suitable metrics (distances, angles) give measures of association or correlation that can be employed in supervised or unsupervised learning tasks. Since volume grows exponentially with space dimension, input data with even a moderate number of features may require a prohibitively large number of samples to yield sufficient data space coverage. This is an example of the so-called *curse of dimensionality*.\n",
    "\n",
    "Usually, not all directions in data space are equally important, so that the data can be well represented using a lower number of possibly redefined features. Identifying the most important directions and thus a more compact data representation is a typical application of *dimensionality reduction*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: PCA of the hand-written digits data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images in the [hand-written digits data set](https://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html) are 8x8 pixel arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = datasets.load_digits(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with such a low resolution, the dimension of the data space is 64. We aim at representing the data in a lower-dimensional space using *Principal Component Analysis (PCA)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, we consider only the two most important directions obtained through PCA. The `fit_transform()` method conveniently produces a fit and projects the data vectors onto the resulting two-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PCA(2)\n",
    "prj = model.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `prj` contains the coordinates of the data vectors in the principal components frame of reference. The distribution of data vectors is visualized by means of a scatter plot as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(10):\n",
    "    ind = y==t\n",
    "    cmap = sns.color_palette('tab10',10)\n",
    "    plt.scatter(prj[ind,0],prj[ind,1],alpha=0.5,s=10,\n",
    "                color=cmap[t],label='{}'.format(t))\n",
    "plt.title('Hand-written digits data: PCA projection')\n",
    "plt.xlabel('Principal component 1')\n",
    "plt.ylabel('Principal component 2')\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatter plot shows that even in this reduced two-dimensional representation, the digit 0 point cloud has very little overlap with the digit 7 point cloud so that we can expect them to be well separated in a classification exercise. Judging from the overlap between the point clouds of digits 2 and 8, they are expected to be more difficult to discern. To double-check these expectations, the classification exercise is carried out in the code cell below. The confusion matrix quantifies the misclassifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianNB()\n",
    "X_trn,X_val,y_trn,y_val = train_test_split(X,y,train_size=0.5,random_state=99)\n",
    "model.fit(X_trn,y_trn)\n",
    "y_prd = model.predict(X_val)\n",
    "conf = confusion_matrix(y_val,y_prd)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.heatmap(conf,cmap='Blues',cbar=False,annot=True)\n",
    "plt.xlabel('Predicted digit')\n",
    "plt.ylabel('True digit');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Explained variance ratios in PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key measure to check the meaningfulness of a reduced PCA representation is the *explained variance ratio*, i.e., the share of the total variance in the data captured by a particular principal component.\n",
    "\n",
    "- After loading the hand-written digits data set, convince yourself that the sum of the first two variance ratios is already above 28%, while the dimensionality of the representation is effectively reduced to 2/64 $\\approx$ 3%.\n",
    "- Compute and plot the explained variance ratios for all principal components as well as their cumulative sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final section of this tutorial notebook reviews and highlights important concepts related to *model validation*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holdout set validation example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the reasoning behind the split into training data and testing data for model validation, let us revisit the iris flower classification example, this time modeled by means of the k-neighbors classifier with `n_neighbors=1`. Here the predicted label is simply the label of the nearest training point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = datasets.load_iris(return_X_y=True)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier(n_neighbors=1)\n",
    "model.fit(X,y)\n",
    "print('Model score using all data : ',model.score(X,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting score of 1.0 suggests, erroneously, an accuracy of 100% and thus a perfect model. The problem stems from a flawed validation approach as the same data are used for training and testing. Using holdout sets, we obtain a more realistic result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trn,X_val,y_trn,y_val = train_test_split(X,y,train_size=0.5,random_state=99)\n",
    "model.fit(X_trn,y_trn)\n",
    "print('Model score using validation data : ',model.score(X_val,y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Underfitting and overfitting, validation and learning curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained, e.g., in [section 5.03 Hyperparameters and Model Validation](https://jakevdp.github.io/PythonDataScienceHandbook/05.03-hyperparameters-and-model-validation.html) of the [Python Data Science Handbook by Jake Vanderplas](https://jakevdp.github.io/PythonDataScienceHandbook/), or in [section 3. Model selection and evaluation](https://scikit-learn.org/stable/model_selection.html) of the [scikit-learn user guide](https://scikit-learn.org/stable/user_guide.html), comparing training scores and validation scores provides guidelines for model selection and validation. Consult the literature to get the full picture. Here we summarize some of the key terminology.\n",
    "\n",
    "- *Model complexity* depends essentially on the number of features or model parameters. Fewer parameters lead to simpler models, more parameters to a higher model complexity.\n",
    "- *Underfitting* results from using a model that is too simple and thus cannot sufficiently account for the variability in the predictable (deterministic) part of the data. In statistical terminology, such models are said to have a *high bias*. For instance, when a large data set is generated from a quartic (degree four) polynomial with small amounts of noise, but a low-order polynomial (e.g., a linear function) is employed as a model, the predictions are likely to fail because of underfitting.\n",
    "- *Overfitting* results from using a model with too many tunable parameters for a data set produced from a simple process contaminated by possibly large-amplitude noise, thus showing variability mostly in the random contribution to the signal. In statistical terminology, such models are characterized by *high variance*. For instance, when a small data set is generated from a linear or a quadratic function with large amounts of noise, but a high-order polynomial is employed as a model, the predictions are likely to fail because of overfitting.\n",
    "- *Validation curves* display model performance (scores) versus model complexity (number of features or parameters). High-bias models tend to produce training and validation scores that are close but low. High-variance models are characterized by high training scores but low validation scores. The best model is likely found in between around the peak of the validation score.\n",
    "- *Learning curves* display model performance (scores) versus the size of the training data set. A training set of insufficient size tends to produce a significant gap between training score and validation score that is typical for high-variance models (overfitting). A good fit is indicated by converging training scores and validation scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: `sklearn.learning_curve`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visit [scikit-learn.org](https://scikit-learn.org/stable/index.html). Consult the [user guide](https://scikit-learn.org/stable/user_guide.html) and suitable [tutorials](https://scikit-learn.org/stable/tutorial/index.html) to learn about validation curves and learning curves as implemented in `sklearn.learning_curve`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution: Classification of the reduced hand-written digits data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = datasets.load_digits(return_X_y=True,n_class=3)\n",
    "### Select the model class.\n",
    "model = GaussianNB()\n",
    "### Provide the data in an appropriate format.\n",
    "X,y = datasets.load_digits(return_X_y=True,n_class=3)\n",
    "### Fit the model to the data.\n",
    "model.fit(X,y)\n",
    "### Compute model predictions and the confusion matrix.\n",
    "yprd = model.predict(X)\n",
    "ConfMat = confusion_matrix(y,yprd)\n",
    "print('Confusion matrix:')\n",
    "print(ConfMat)\n",
    "### Model (cross-)validation.\n",
    "cv_scores = cross_val_score(model,X,y,cv=5)\n",
    "print('\\nCross-validation scores:')\n",
    "print(cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution: Explained variance ratios in PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = datasets.load_digits(return_X_y=True)\n",
    "model = PCA(2).fit(X)\n",
    "print('PCA1 and PCA2, explained variance ratios    : ',model.explained_variance_ratio_)\n",
    "print('Sum of explained variance ratios in percent : ',100*np.sum(model.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PCA().fit(X)\n",
    "plt.title('Hand-written digits data: PCA variance ratios')\n",
    "plt.plot(model.explained_variance_ratio_,label='Explained Variance Ratio (EVR)')\n",
    "plt.plot(np.cumsum(model.explained_variance_ratio_),label='Cumulative EVR')\n",
    "plt.xlabel('Number of principal components')\n",
    "plt.ylabel('EVR / Cumulative EVR')\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
